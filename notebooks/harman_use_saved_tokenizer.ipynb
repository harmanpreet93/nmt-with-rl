{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/notAlex2/Translation-Team08-IFT6759/blob/master/notebooks/harman_use_saved_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TrBC_yz4mpj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "project_path = \"/home/alex/Translation-Team08-IFT6759\"\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6NNccnH8lTy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers===2.7.0\n",
      "  Downloading transformers-2.7.0-py3-none-any.whl (544 kB)\n",
      "\u001b[K     |████████████████████████████████| 544 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (0.0.38)\n",
      "Requirement already satisfied: numpy in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (1.18.2)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (2020.2.20)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Using cached dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: boto3 in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (1.12.30)\n",
      "Requirement already satisfied: sentencepiece in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (0.1.85)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (4.43.0)\n",
      "Requirement already satisfied: requests in /home/alex/tr_env/lib/python3.6/site-packages (from transformers===2.7.0) (2.23.0)\n",
      "Requirement already satisfied: six in /home/alex/tr_env/lib/python3.6/site-packages (from sacremoses->transformers===2.7.0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/alex/tr_env/lib/python3.6/site-packages (from sacremoses->transformers===2.7.0) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/alex/tr_env/lib/python3.6/site-packages (from sacremoses->transformers===2.7.0) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.30 in /home/alex/tr_env/lib/python3.6/site-packages (from boto3->transformers===2.7.0) (1.15.30)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/alex/tr_env/lib/python3.6/site-packages (from boto3->transformers===2.7.0) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/alex/tr_env/lib/python3.6/site-packages (from boto3->transformers===2.7.0) (0.3.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/alex/tr_env/lib/python3.6/site-packages (from requests->transformers===2.7.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/alex/tr_env/lib/python3.6/site-packages (from requests->transformers===2.7.0) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/alex/tr_env/lib/python3.6/site-packages (from requests->transformers===2.7.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alex/tr_env/lib/python3.6/site-packages (from requests->transformers===2.7.0) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/alex/tr_env/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.30->boto3->transformers===2.7.0) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/alex/tr_env/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.30->boto3->transformers===2.7.0) (0.15.2)\n",
      "Installing collected packages: dataclasses, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.6.0\n",
      "    Uninstalling transformers-2.6.0:\n",
      "      Successfully uninstalled transformers-2.6.0\n",
      "Successfully installed dataclasses-0.7 transformers-2.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers===2.7.0 \n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "9Q3HVdyd5S6M",
    "outputId": "0eb69f84-8a01-443f-e41e-50b128c2459a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;42mtokenizer_data_en\u001b[00m\r\n",
      "├── config.json\r\n",
      "├── merges.txt\r\n",
      "├── special_tokens_map.json\r\n",
      "├── tokenizer_config.json\r\n",
      "└── vocab.json\r\n",
      "\r\n",
      "0 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path_en = \"tokenizer_data_en\"\n",
    "! tree tokenizer_data_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "hqeuJMAa7nmR",
    "outputId": "68c5db6f-7795-4926-cf60-71285332ccdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;42mtokenizer_data_fr\u001b[00m\r\n",
      "├── config.json\r\n",
      "├── merges.txt\r\n",
      "├── special_tokens_map.json\r\n",
      "├── tokenizer_config.json\r\n",
      "└── vocab.json\r\n",
      "\r\n",
      "0 directories, 5 files\r\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path_fr = \"tokenizer_data_fr\"\n",
    "! tree tokenizer_data_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yk71RxoR7d-M"
   },
   "source": [
    "### **Load/Restore Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUTb-PPj6BrE"
   },
   "outputs": [],
   "source": [
    "# make sure the path contains above 5 files\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(tokenizer_path_en)\n",
    "\n",
    "tokenizer_fr = AutoTokenizer.from_pretrained(tokenizer_path_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFlRH311JJRb"
   },
   "source": [
    "### How to use Tokenizer  \n",
    "\n",
    "\n",
    "*   To tokenize a sentence, use `tokens = tokenizer.tokenize(sentence)`\n",
    "*   To encode a sentence to integers, use `encoded_sequence = tokenizer.encode(sentence)`. Not that it also adds start and end tokens, i.e. `<s>` and `</s>` to the encoded outputs.\n",
    "*   To decode/untokenize a sentence, use `tokenizer.decode(encoded_sequence, skip_special_tokens=True)`\n",
    "* We use keras's `pad_sequences` to pad. Make sure to use `tokenizer.pad_token_id` to provide the tokenizer specific pad token. \n",
    "\n",
    "\n",
    "Usage of this tokenizer is shown in following examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XZf0htpZuwzp",
    "outputId": "5d686fcd-ce16-4b23-839b-ccc1c0f3f67c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'ont', 'real', 'Ġis', 'Ġa', 'Ġgreat', 'Ġcity', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Montreal is a great city.\".strip()\n",
    "tokenizer_en.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwR5eKxgPjbr"
   },
   "source": [
    "Capitalization and lowercased inputs will give different results. Hence, its user's choice on how he provides input to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZcbFWJP2Pgem",
    "outputId": "554a1c4d-2e83-4861-d94c-1c77bf76f195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mont', 'real', 'Ġis', 'Ġa', 'Ġgreat', 'Ġcity', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Montreal is a great city.\".strip().lower()\n",
    "tokenizer_en.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zNaQ3iQoD9Qx",
    "outputId": "0ac63b37-b612-4af4-be1e-61c477e6a317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 18304, 306, 263, 803, 3194, 18, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_seq = tokenizer_en.encode(text)\n",
    "encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G09sii-l26VL",
    "outputId": "4641de8b-b201-4de6-9a0e-6be2e2d18f27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> montreal is a great city.</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode sequence back!\n",
    "tokenizer_en.decode(encoded_seq, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MFraHntzFJxs",
    "outputId": "5b649663-ba6a-4440-b822-9b8b060fae02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' montreal is a great city.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(encoded_seq, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "onbcSMDv9aNH",
    "outputId": "bda79186-064a-4f2c-ac72-c2da89ebdfda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 18304, 306, 263, 803, 3194, 18, 2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer_en.encode_plus(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-JqhRyIosiJ4",
    "outputId": "242b2cce-11a0-488c-80fa-18878e405231"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 18304, 306, 263, 803, 3194, 18, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IvsD2BJkRn13",
    "outputId": "92e3ae54-c7ca-4c9b-bd1d-bc18527358f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.get_special_tokens_mask(encoded_seq, already_has_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V5WgNfinsiHr",
    "outputId": "0c54c384-a234-4563-fa2f-8edca7016b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1, 18304,   306,   263,   803,  3194,    18,     2,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# pad sequences!\n",
    "padded_seq = pad_sequences([tokens[\"input_ids\"]], padding='post', value=tokenizer_en.pad_token_id, maxlen=15)\n",
    "padded_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FL0weZm9siGV",
    "outputId": "ac88e76d-c362-42cd-9c56-45381e1f242c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.get_special_tokens_mask(padded_seq[0], already_has_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IGlV7YcSXEN"
   },
   "source": [
    "#### Un-tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3IDsuZlexIQK",
    "outputId": "5eeaf939-e8e6-44e0-bf8d-94a41a95d77b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> montreal is a great city.</s><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(padded_seq[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ldWmAe6oxIKx",
    "outputId": "ff868a86-a750-4002-a057-120956a45282"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' montreal is a great city.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(padded_seq[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xRZPf7wOJOqM",
    "outputId": "bba62ec6-253e-45f2-bff4-829dfc10bc98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 225, 49, 2095, 9337, 306, 263, 803, 3194, 2],\n",
       "  [1, 225, 39, 289, 8974, 407, 793, 5869, 2]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode batch in one go!\n",
    "text1 = \"Montreal is a great city\".strip()\n",
    "text2 = \"California has good weather\".strip()\n",
    "\n",
    "texts = [text1, text2]\n",
    "tokenizer_en.batch_encode_plus(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alex's Test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang1 = \"but if you do this it 's quick\"   # train.lang1 format\n",
    "unaligned_en = \"But if you do this, it's quick.\" # unaligned.en format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'Ġif', 'Ġyou', 'Ġdo', 'Ġthis', 'Ġit', \"Ġ'\", 's', 'Ġquick']\n",
      "['B', 'ut', 'Ġif', 'Ġyou', 'Ġdo', 'Ġthis', ',', 'Ġit', \"'s\", 'Ġquick', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_en.tokenize(train_lang1))\n",
    "print(tokenizer_en.tokenize(unaligned_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> but if you do this it's quick</s>\n",
      "<s> But if you do this, it's quick.</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_en.decode(tokenizer_en.encode(train_lang1)))\n",
    "print(tokenizer_en.decode(tokenizer_en.encode(unaligned_en)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Capitalization needs to be removed from \"unaligned.en\" before tokenization\n",
    "* Punctuation needs to be removed from \"unaligned.en\" before tokenization\n",
    "* Punctuation is concatenated on output (it's VS it 's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alors', 'Ġ,', 'ĠoÃ¹', 'Ġen', 'Ġsommes', 'Ġ-', 'Ġnous', 'Ġ?']\n",
      "['Alors', ',', 'ĠoÃ¹', 'Ġen', 'Ġsommes', '-', 'nous', '?']\n"
     ]
    }
   ],
   "source": [
    "train_lang2 = \"Alors , où en sommes - nous ?\" # train.lang2 format\n",
    "unaligned_fr = \"Alors, où en sommes-nous?\"    # unaligned.fr format\n",
    "\n",
    "print(tokenizer_fr.tokenize(train_lang2))   \n",
    "print(tokenizer_fr.tokenize(unaligned_fr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Alors, où en sommes - nous?</s>\n",
      "<s> Alors, où en sommes-nous?</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fr.decode(tokenizer_fr.encode(train_lang2)))\n",
    "print(tokenizer_fr.decode(tokenizer_fr.encode(unaligned_fr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Tokenizer concatenates punctuation when decoding (nous? VS nous ?)\n",
    "* unaligned.fr needs to have tokens disconnected from words before fitting tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10706, 2]\n",
      "[1, 1282, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fr.encode(\"Alors\"))\n",
    "print(tokenizer_fr.encode(\"alors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', \"'\", 'Ġai', 'Ġ?']\n",
      "['J', \"'\", 'ai', '?']\n",
      "<s> J' ai?</s>\n",
      "<s> J'ai?</s>\n"
     ]
    }
   ],
   "source": [
    "train_lang2 = \"J' ai ?\" # train.lang2 format\n",
    "unaligned_fr = \"J'ai?\"  # unaligned.fr format\n",
    "\n",
    "print(tokenizer_fr.tokenize(train_lang2))   \n",
    "print(tokenizer_fr.tokenize(unaligned_fr))\n",
    "print(tokenizer_fr.decode(tokenizer_fr.encode(train_lang2)))\n",
    "print(tokenizer_fr.decode(tokenizer_fr.encode(unaligned_fr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Capitalization affects token definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add/remove capitalization special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_cap_tokens(tokens):\n",
    "    output = tokens.strip()\n",
    "    positions = []\n",
    "    for m in re.finditer(r'\\b[A-ZÀ-ÖÙ-Ý]', output):\n",
    "        positions += [m.start()]\n",
    "    \n",
    "    for idx in reversed(positions):\n",
    "        output = output[:idx] + '@ ' + output[idx].lower() + output[(idx+1):]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ test @ words\n",
      "test\n",
      "@ t @ t @ t @ tT\n",
      "@ t   @ t     @ t    @ tT\n",
      "@ état\n",
      "@ a @ z @ à @ ö @ ù @ ý\n",
      "@ aZÀÖÙÝ\n"
     ]
    }
   ],
   "source": [
    "print(add_cap_tokens(\"Test Words\"))\n",
    "print(add_cap_tokens(\"test\"))\n",
    "print(add_cap_tokens(\"T T T TT\"))\n",
    "print(add_cap_tokens(\"   T   T     T    TT    \"))\n",
    "print(add_cap_tokens(\"État\"))\n",
    "print(add_cap_tokens(\"A Z À Ö Ù Ý\"))\n",
    "print(add_cap_tokens(\"AZÀÖÙÝ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cap_tokens(tokens):\n",
    "    output = re.sub(' +', ' ', tokens).strip()\n",
    "    positions = []\n",
    "    for m in re.finditer(r'@', output):\n",
    "        positions += [m.start()]\n",
    "    \n",
    "    for idx in reversed(positions):\n",
    "        # Catch the case when sequence is terminated by <CAP>\n",
    "        if idx + 2 >= len(output):\n",
    "            output = output[:idx]\n",
    "            continue\n",
    "        output = output[:idx] + output[(idx+2)].upper() + output[(idx+3):]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Words\n",
      "test\n",
      "T T T TT\n",
      "T T T TT\n",
      "État\n",
      "A Z À Ö Ù Ý\n",
      "AZÀÖÙÝ\n",
      "AZÀÖÙÝ <CAP>\n",
      ". AZÀÖÙÝ \n",
      "AZÀÖÙÝ \n",
      "a . a\n"
     ]
    }
   ],
   "source": [
    "print(remove_cap_tokens(\"@ test @ words\"))\n",
    "print(remove_cap_tokens(\"test\"))\n",
    "print(remove_cap_tokens(\"@ t @ t @ t @ tT\"))\n",
    "print(remove_cap_tokens(\"@ t @ t @ t @ tT\"))\n",
    "print(remove_cap_tokens(\"@ état\"))\n",
    "print(remove_cap_tokens(\"@ a @ z @ à @ ö @ ù @ ý\"))\n",
    "print(remove_cap_tokens(\"@ aZÀÖÙÝ\"))\n",
    "print(remove_cap_tokens(\"@ aZÀÖÙÝ <CAP> \"))\n",
    "print(remove_cap_tokens(\"@ @ @ . @ aZÀÖÙÝ @ @\"))\n",
    "print(remove_cap_tokens(\"@   aZÀÖÙÝ @    \"))\n",
    "print(remove_cap_tokens(\"a @ @ @ @ @ @   . a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2294, 1738, 2]\n",
      "[1, 225, 36, 2294, 225, 36, 1738, 2]\n",
      "test words\n",
      "Test Words\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en.add_tokens(['@'])\n",
    "tokenizer_fr.add_tokens(['@'])\n",
    "\n",
    "print(tokenizer_en.encode(add_cap_tokens(\"test words\")))\n",
    "print(tokenizer_en.encode(add_cap_tokens(\"Test Words\")))\n",
    "\n",
    "print(remove_cap_tokens(tokenizer_en.decode(tokenizer_en.encode(add_cap_tokens(\"test words\")), skip_special_tokens=True)))\n",
    "print(remove_cap_tokens(tokenizer_en.decode(tokenizer_en.encode(add_cap_tokens(\"Test Words\")), skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Files Before Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_caps = \"/home/alex/Translation-Team08-IFT6759/data/train.fr.tokenized/unaligned.fr\"\n",
    "with open(file_with_caps) as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/home/alex/Translation-Team08-IFT6759/data/train.fr.tokenized/unaligned.fr.CAP\"\n",
    "with open(output_file, \"w+\") as f:\n",
    "    for line in lines:\n",
    "        f.write(add_cap_tokens(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_with_caps = \"/home/alex/Translation-Team08-IFT6759/data/train.lang2\"\n",
    "with open(file_with_caps) as f:\n",
    "    lines = [line.rstrip('\\n') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/home/alex/Translation-Team08-IFT6759/data/train.lang2.CAP\"\n",
    "with open(output_file, \"w+\") as f:\n",
    "    for line in lines:\n",
    "        f.write(add_cap_tokens(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-concatenate punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: punctuation is concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alors', 'Ġ,', 'ĠoÃ¹', 'Ġen', 'Ġsommes', 'Ġ-', 'Ġnous', 'Ġ?']\n",
      "<s> Alors, où en sommes - nous?</s>\n"
     ]
    }
   ],
   "source": [
    "train_lang2 = \"Alors , où en sommes - nous ?\" # train.lang2 format\n",
    "\n",
    "print(tokenizer_fr.tokenize(train_lang2))   \n",
    "print(tokenizer_fr.decode(tokenizer_fr.encode(train_lang2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: skip tokenizer clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alors , où en sommes - nous ?\n"
     ]
    }
   ],
   "source": [
    "train_lang2 = \"Alors , où en sommes - nous ?\" # train.lang2 format\n",
    "\n",
    "output = tokenizer_fr.decode(tokenizer_fr.encode(train_lang2), clean_up_tokenization_spaces=False, skip_special_tokens=True).strip()\n",
    "print(output)\n",
    "assert output == train_lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Add processed version of unaligned en/fr files"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPy3B1CMQbPH59mDClDuMIX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1lyJE8TKGyasgfBDiITWVRWQztwItSODY",
   "name": "harman_use_saved_tokenizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
