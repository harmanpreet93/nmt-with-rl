{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harman_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18TEGLm-JgwXfA08oxbYWCN2AZpyCmHRu",
      "authorship_tag": "ABX9TyO8YOG8dNXGtcuTaKUFQiSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notAlex2/Translation-Team08-IFT6759/blob/master/notebooks/harman_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NetP1bhU2MS",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains code on how to train Byte-Pair tokenizer on your own dataset. Byte-Pair encoding handles Out-Of-Vocabulary words as well!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TrBC_yz4mpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "project_path = \"/content/drive/My Drive/machine-translation\"\n",
        "os.chdir(project_path)\n",
        "\n",
        "data_path = os.path.join(project_path, 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NNccnH8lTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers===2.7.0 \n",
        "\n",
        "try:\n",
        "    import tokenizers\n",
        "except ImportError as e:\n",
        "    ! pip install tokenizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNCxDxh8rQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def set_seed(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXjf4iS0KLOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_tokenizer(save_tokenizer_path: str,\n",
        "                    training_files: str,\n",
        "                    special_tokens, \n",
        "                    min_frequency: int,\n",
        "                    lowercase: bool,\n",
        "                    VOCAB_SIZE: int) -> None:\n",
        "\n",
        "    # Need to train tokenizer only once\n",
        "    # create vocab from scratch\n",
        "    bpe_tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=lowercase)\n",
        "\n",
        "    bpe_tokenizer.train(\n",
        "                    files=training_files, \n",
        "                    vocab_size=VOCAB_SIZE, \n",
        "                    min_frequency=min_frequency, \n",
        "                    show_progress=True,\n",
        "                    special_tokens=special_tokens\n",
        "                    )\n",
        "\n",
        "    if not os.path.exists(save_tokenizer_path):\n",
        "        os.makedirs(save_tokenizer_path)\n",
        "    \n",
        "    # This saves 2 files, which are required later by the tokenizer: merges.txt and vocab.json\n",
        "    bpe_tokenizer.save(save_tokenizer_path)\n",
        "    \n",
        "    model_type=\"roberta\" # roBERTa model is better than BERT for language modelling\n",
        "    config = {\n",
        "        \"vocab_size\": VOCAB_SIZE,\n",
        "        \"model_type\": model_type \n",
        "        }\n",
        "\n",
        "    config_path = os.path.join(save_tokenizer_path, \"config.json\")\n",
        "    with open(config_path, 'w') as fp:\n",
        "        json.dump(config, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Y3jr1CM0Dr",
        "colab_type": "text"
      },
      "source": [
        "### Train Tokenizer using vocab files\n",
        "\n",
        "To train a new tokenizer, you need to provide \n",
        "* list of data-files on which you want to train your tokenizer\n",
        "* special tokens such as `\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>`\n",
        "* vocabulary size\n",
        "* min_frequency int\n",
        "* lowercase boolean\n",
        "* path to save your tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipg3Vi4ZLIvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7329f7f-3bc6-47b9-c3e1-61c83254b6aa"
      },
      "source": [
        "VOCAB_SIZE = 60000\n",
        "min_frequency = 2\n",
        "lowercase = True\n",
        "special_tokens = [\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]\n",
        "unaligned_en_path = os.path.join(data_path, 'unaligned.en')\n",
        "aligned_en_path = os.path.join(data_path, 'train.lang1')\n",
        "training_files = [unaligned_en_path, aligned_en_path]\n",
        "pretrained_tokenizer_path = \"tokenizer_data\"\n",
        "\n",
        "ALREADY_TRAINED_ONCE = True\n",
        "if ALREADY_TRAINED_ONCE:\n",
        "    print(\"Tokenizer already trained. Set ALREADY_TRAINED_ONCE=False to re-train tokenizer\")\n",
        "else:\n",
        "    # train your Byte Pair Tokenizer on your own training files!\n",
        "    train_tokenizer(pretrained_tokenizer_path, \n",
        "                    training_files, \n",
        "                    special_tokens, \n",
        "                    min_frequency, \n",
        "                    lowercase, \n",
        "                    VOCAB_SIZE)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer already trained. Set ALREADY_TRAINED_ONCE=False to re-train tokenizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeYRoGBJGyP",
        "colab_type": "text"
      },
      "source": [
        "### Load Saved Tokenizer \n",
        "The tokenizer is already trained before this point. The output of trained tokenizer function (`train_tokenizer(args)`) is 3 files - `merges.txt, vocab.txt and config.json`. Please note that you only have to train the tokenizer once.  \n",
        "\n",
        "To load the tokenizer set `pretrained_tokenizer_path` to the folder where above 3 mentioned files are saved. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_B3j8L4JFwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# make sure the path contains 3 files: config.json, merges.txt and vocab.json\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path, cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlRH311JJRb",
        "colab_type": "text"
      },
      "source": [
        "### How to use Tokenizer  \n",
        "\n",
        "\n",
        "*   To tokenize a sentence, use `tokens = tokenizer.tokenize(sentence)`\n",
        "*   To encode a sentence to integers, use `encoded_sequence = tokenizer.encode(sentence)`. Not that it also adds start and end tokens, i.e. `<s>` and `</s>` to the encoded outputs.\n",
        "*   To decode/untokenize a sentence, use `tokenizer.decode(encoded_sequence, skip_special_tokens=True)`\n",
        "* Padding isn't supported inherently in this tokenizer. Hence, we use keras's `pad_sequences` to pad. Make sure to use `tokenizer.pad_token_id` to provide the tokenizer specific pad token. \n",
        "\n",
        "\n",
        "Usage of this tokenizer is shown in following examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZf0htpZuwzp",
        "colab_type": "code",
        "outputId": "1bd8e102-9223-440f-8f87-35556be12cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = \"Montreal is a great city\".strip()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['M', 'ont', 'real', 'Ġis', 'Ġa', 'Ġgreat', 'Ġcity']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwR5eKxgPjbr",
        "colab_type": "text"
      },
      "source": [
        "Capitalization and lowercased inputs will give different results. Hence, its user's choice on how he provides input to the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcbFWJP2Pgem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fe49440-728a-416b-852d-cb3283ca1837"
      },
      "source": [
        "text = \"Montreal is a great city\".strip().lower()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mont', 'real', 'Ġis', 'Ġa', 'Ġgreat', 'Ġcity']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b6331ec6-01b0-48f3-c417-6a03c5241dc8",
        "id": "zNaQ3iQoD9Qx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoded_seq = tokenizer.encode(text)\n",
        "encoded_seq"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 18325, 306, 263, 805, 3195, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G09sii-l26VL",
        "colab_type": "code",
        "outputId": "62cfc254-8a7f-4815-f159-a40aff286bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# decode sequence back!\n",
        "tokenizer.decode(encoded_seq, skip_special_tokens=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> montreal is a great city</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFraHntzFJxs",
        "colab_type": "code",
        "outputId": "642da739-c32f-4b54-9c8d-2b02462345d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.decode(encoded_seq, skip_special_tokens=True).strip()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'montreal is a great city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrN-miCauNjI",
        "colab_type": "code",
        "outputId": "6454e6af-3f48-4e6c-eba6-290f8dd09c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "tokens = tokenizer.encode_plus(text)\n",
        "tokens"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
              " 'input_ids': [0, 18325, 306, 263, 805, 3195, 2]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JqhRyIosiJ4",
        "colab_type": "code",
        "outputId": "96d81e98-4db4-41bd-cd73-face974bb045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens[\"input_ids\"]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 18325, 306, 263, 805, 3195, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxiZ6u9GsiLR",
        "colab_type": "code",
        "outputId": "95ee96ee-3fda-43b8-d288-4f5435006bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.get_special_tokens_mask(tokens[\"input_ids\"], already_has_special_tokens=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvsD2BJkRn13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "972c9c3c-22a5-47aa-dc0f-bcf869255174"
      },
      "source": [
        "tokenizer.get_special_tokens_mask(encoded_seq, already_has_special_tokens=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5WgNfinsiHr",
        "colab_type": "code",
        "outputId": "1e487ecf-ce68-4e74-e2b2-63698d912974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# pad sequences!\n",
        "padded_seq = pad_sequences([tokens[\"input_ids\"]], padding='post', value=tokenizer.pad_token_id, maxlen=15)\n",
        "padded_seq[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0, 18325,   306,   263,   805,  3195,     2,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL0weZm9siGV",
        "colab_type": "code",
        "outputId": "6109140c-e3cd-49a7-9a69-2b36cfc7c63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.get_special_tokens_mask(padded_seq[0], already_has_special_tokens=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IGlV7YcSXEN",
        "colab_type": "text"
      },
      "source": [
        "#### Un-tokenize inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDsuZlexIQK",
        "colab_type": "code",
        "outputId": "e4e64dfe-5c96-44c1-93d0-eaff5b3e8d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.decode(padded_seq[0], skip_special_tokens=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Montreal is a great city</s><pad><pad><pad><pad><pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWmAe6oxIKx",
        "colab_type": "code",
        "outputId": "57e20633-76c7-4bad-ece2-a9a1c59ff2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.decode(padded_seq[0], skip_special_tokens=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Montreal is a great city'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e34d7fdc-45b3-4abf-bad6-e8385c5e57e8",
        "id": "xRZPf7wOJOqM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# encode batch in one go!\n",
        "text1 = \"Montreal is a great city\".strip()\n",
        "text2 = \"California has good weather\".strip()\n",
        "\n",
        "texts = [text1, text2]\n",
        "tokenizer.batch_encode_plus(texts)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "  [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              " 'input_ids': [[0, 225, 49, 2096, 9317, 306, 263, 805, 3195, 2],\n",
              "  [0, 225, 39, 289, 8955, 407, 793, 5872, 2]]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaeRewylSEjR",
        "colab_type": "text"
      },
      "source": [
        "### Use of Tokenizer with `tf.Dataset` Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RPQgoi667q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datalaoder stuff\n",
        "def tokenize_string(tokenizer, raw_string):\n",
        "    return (raw_string)\n",
        "  \n",
        "def encode_dataset(file_path, pretrained_tokenizer_path, lowercase, num_examples):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path)\n",
        "    sentences = io.open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    if lowercase:\n",
        "        encoded_sequences = [tokenizer.encode(sentence.lower()) for sentence in sentences[:num_examples]]\n",
        "    else:\n",
        "        encoded_sequences = [tokenizer.encode(sentence) for sentence in sentences[:num_examples]]\n",
        "\n",
        "    encoded_sequences = pad_sequences(encoded_sequences, padding='post', value=tokenizer.pad_token_id)\n",
        "    decoded_sequences = [tokenizer.decode(x) for x in encoded_sequences]\n",
        "    return encoded_sequences, decoded_sequences\n",
        "\n",
        "\n",
        "def data_generator_fn():\n",
        "    NUM_EXAMPLES = 10\n",
        "    lowercase = True\n",
        "    encoded_sequences, decoded_sequences = encode_dataset(unaligned_en_path,\n",
        "                                                          pretrained_tokenizer_path, \n",
        "                                                          lowercase, \n",
        "                                                          NUM_EXAMPLES)\n",
        "    \n",
        "    for enc_seq, decoded_seq in zip(encoded_sequences, decoded_sequences):\n",
        "        yield enc_seq, decoded_seq\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "# dataset object\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator_fn,\n",
        "    output_types=(tf.int32, tf.string)\n",
        "    ).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NUVxzlx3-5Z",
        "colab_type": "code",
        "outputId": "9d2203e9-2a34-469b-f1d9-b1f4461c804b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "for enc, dec in dataset.take(1):\n",
        "    print(enc)\n",
        "    print(dec)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    0   325   266  1354  4876   288   266  8427   324   648   728  1102\n",
            "  14787    16  1372    16  3012    16  1397   299  2886    17 13103    18\n",
            "    330   411  1819    18     2     1     1     1     1     1     1     1]\n",
            " [    0   853    75   728   491   544  3766   266  2506  3470    16   698\n",
            "    314   438   266   767   478  5719  4873   282  3769  1414    16  5122\n",
            "    352   290 22882   325   500 22840 22578   547    87  2976    18     2]], shape=(2, 36), dtype=int32)\n",
            "tf.Tensor(\n",
            "[b\"<s> for the second phase of the trials we just had different sizes, small, medium, large and extra-large. it's true.</s><pad><pad><pad><pad><pad><pad><pad>\"\n",
            " b'<s> geng had been my host the previous january, when i was the first us defense secretary to visit china, acting as an interlocutor for president jimmy carter\\xe2\\x80\\x99s administration.</s>'], shape=(2,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT7jHj57IMtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}