{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "harman_bert_language_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aH2xQyaK7N4_gQM6I7mL3KCj2hWAKdp4",
      "authorship_tag": "ABX9TyOm2QxYjmBq01Y2f45OiyQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notAlex2/Translation-Team08-IFT6759/blob/master/notebooks/harman_bert_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TrBC_yz4mpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "project_path = \"/content/drive/My Drive/machine-translation\"\n",
        "os.chdir(project_path)\n",
        "\n",
        "data_path = os.path.join(project_path, 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6NNccnH8lTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNCxDxh8rQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import tokenizers\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import (\n",
        "    PreTrainedTokenizer, \n",
        "    AutoTokenizer, \n",
        "    BertConfig, \n",
        "    TFBertModel\n",
        ")\n",
        "                          \n",
        "from typing import Tuple\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(10)\n",
        "unaligned_en_path = os.path.join(data_path, 'unaligned.en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAWv71gzRAE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_EXIST = True\n",
        "VOCAB_SIZE = 30000\n",
        "pretrained_tokenizer_path = \"/content/drive/My Drive/machine-translation/transBERTo\"\n",
        "tokenizer_name = \"bpe_tokenizer\"\n",
        "\n",
        "if not VOCAB_EXIST:\n",
        "    # create vocab from scratch\n",
        "    bpe_tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
        "    special_tokens = [\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"]\n",
        "\n",
        "    bpe_tokenizer.train(\n",
        "                    files=[unaligned_en_path], \n",
        "                    vocab_size=VOCAB_SIZE, \n",
        "                    min_frequency=2, \n",
        "                    show_progress=True,\n",
        "                    special_tokens=special_tokens\n",
        "                    )\n",
        "\n",
        "    # Save files to disk\n",
        "    bpe_tokenizer.save(pretrained_tokenizer_path, tokenizer_name)\n",
        "    \n",
        "    # rename files to vocab.json and merges.txt\n",
        "    src = os.path.join(pretrained_tokenizer_path, tokenizer_name + \"-vocab.json\")\n",
        "    dst = os.path.join(pretrained_tokenizer_path, \"vocab.json\")\n",
        "    os.rename(src, dst)\n",
        "\n",
        "    src = os.path.join(pretrained_tokenizer_path, tokenizer_name + \"-merges.txt\")\n",
        "    dst = os.path.join(pretrained_tokenizer_path, \"merges.txt\")\n",
        "    os.rename(src, dst)\n",
        "\n",
        "config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.3,\n",
        "  \"hidden_size\": 128,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"num_attention_heads\": 1,\n",
        "  \"num_hidden_layers\": 1,\n",
        "  \"vocab_size\": VOCAB_SIZE,\n",
        "  \"intermediate_size\": 256,\n",
        "  \"max_position_embeddings\": 256, \n",
        "  \"model_type\": \"roberta\" # roBERTa model is better than BERT for language modelling\n",
        "}\n",
        "\n",
        "config_path = os.path.join(pretrained_tokenizer_path, \"config.json\")\n",
        "with open(config_path, 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path, cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztn3BrwD2YKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_tokens(inputs: tf.Tensor, \n",
        "                tokenizer: PreTrainedTokenizer, \n",
        "                mlm_probability=0.15) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "\n",
        "    if tokenizer.mask_token is None:\n",
        "        raise ValueError(\n",
        "            \"This tokenizer does not have a mask token which is necessary for masked language modeling\"\n",
        "        )\n",
        "    \n",
        "    labels = inputs # ideally this should be inputs.clone()\n",
        "\n",
        "    # We sample a few tokens in each sequence for masked-LM training \n",
        "    # (with probability mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = np.full(labels.shape, mlm_probability)\n",
        "    # special_tokens_mask = [\n",
        "    #         tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.numpy().tolist()\n",
        "    #     ]\n",
        "    special_tokens_mask = tokenizer.get_special_tokens_mask(inputs, already_has_special_tokens=True)\n",
        "    probability_matrix = np.ma.array(probability_matrix, mask=special_tokens_mask).filled(0)\n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = (labels == tokenizer.pad_token_id)\n",
        "        probability_matrix = np.ma.array(probability_matrix, mask=padding_mask).filled(0)\n",
        "\n",
        "    attention_masks = probability_matrix\n",
        "    attention_masks[attention_masks > 0] = 1\n",
        "    masked_indices = tf.compat.v1.distributions.Bernoulli(probs=probability_matrix)\n",
        "    masked_indices = masked_indices.sample(1)[0].numpy()\n",
        "    masked_indices = np.array(masked_indices, dtype=np.bool)\n",
        "    # labels = labels.numpy() \n",
        "    # labels[~masked_indices] = -100 # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    choose_mask_tokens = tf.compat.v1.distributions.Bernoulli(probs=np.full(labels.shape, 0.8))\n",
        "    choose_mask_tokens = choose_mask_tokens.sample(1)[0].numpy()\n",
        "    choose_mask_tokens = np.array(choose_mask_tokens, dtype=np.bool)\n",
        "    indices_replaced = choose_mask_tokens & masked_indices\n",
        "    inputs = inputs.numpy()\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    choose_random_tokens = tf.compat.v1.distributions.Bernoulli(probs=np.full(labels.shape, 0.5))\n",
        "    choose_random_tokens = choose_random_tokens.sample(1)[0].numpy()\n",
        "    choose_random_tokens = np.array(choose_random_tokens, dtype=np.bool)\n",
        "    indices_random = choose_random_tokens & masked_indices & ~indices_replaced\n",
        "    indices_random, choose_random_tokens\n",
        "    random_words = np.random.randint(0, len(tokenizer), labels.shape, dtype=np.int32)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, attention_masks, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RPQgoi667q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# datalaoder stuff\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize_string(tokenizer, raw_string):\n",
        "    return tokenizer.encode(raw_string)\n",
        "  \n",
        "def encode_dataset(file_path, pretrained_tokenizer_path, num_examples):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_path)\n",
        "    sentences = io.open(file_path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    encoded_sequences = [tokenize_string(tokenizer, sentence) for sentence in sentences[:num_examples]]\n",
        "    encoded_sequences = pad_sequences(encoded_sequences, padding='post', value=tokenizer.pad_token_id)\n",
        "    return encoded_sequences\n",
        "\n",
        "NUM_EXAMPLES = 6 \n",
        "BATCH_SIZE = 2\n",
        "encoded_sequences = encode_dataset(unaligned_en_path, pretrained_tokenizer_path, NUM_EXAMPLES)\n",
        "\n",
        "def data_generator_fn():\n",
        "    for seq in encoded_sequences:\n",
        "        inputs = tf.convert_to_tensor(seq)\n",
        "        inputs, attention_masks, masked_labels =  mask_tokens(inputs, tokenizer, mlm_probability=0.15)\n",
        "        yield (inputs, attention_masks), masked_labels\n",
        "\n",
        "# dataset object\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator_fn,\n",
        "    output_types=((tf.int32, tf.int32), tf.int32)\n",
        "    ).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NUVxzlx3-5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "91c740da-9619-4587-fcb7-6d1a8733fd55"
      },
      "source": [
        "for inputs in dataset.take(1):\n",
        "    print(inputs)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((<tf.Tensor: shape=(2, 71), dtype=int32, numpy=\n",
            "array([[  0,   4, 527, 320,   4,   4, 156, 264,   4,   4,   4, 490,   4,\n",
            "          4,   4,   4,   4,   4, 239,   4,  54,   4,   4,   4,   4,   4,\n",
            "         16,   4,   4,  54,   4,   4,   4,   4, 401,   4, 342,   4, 504,\n",
            "        652,   4,   4,   4,   4,   4,   2,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1],\n",
            "       [  0,   4,   4,   4,   4, 655,   4,   4,   4,   4,   4,   4,   4,\n",
            "          4,   4, 441,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
            "          4,   4, 340,   4,   4,   4,  51,   4,   4,   4,   4, 366,   4,\n",
            "          4,   4,   4,   4,   4, 268,   4,   4,   4,   4,   4,   4,   4,\n",
            "          4,   4,   4, 422, 534,   4,   4,   4,   4, 355,   4, 587,   4,\n",
            "          4,   4,  48,   4,   4,   2]], dtype=int32)>, <tf.Tensor: shape=(2, 71), dtype=int32, numpy=\n",
            "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0],\n",
            "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0]], dtype=int32)>), <tf.Tensor: shape=(2, 71), dtype=int32, numpy=\n",
            "array([[  0, 475, 286, 264, 716, 619, 290, 264, 483, 550,  87, 313, 225,\n",
            "         78, 427, 648, 308, 353,  74, 540,  82,  88, 271,  77,  94, 282,\n",
            "         16, 602,  16, 723,  16, 644, 297, 387, 401,  69,  17,  80, 504,\n",
            "         18, 330,  88, 389, 712,  18,   2,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "          1,   1,   1,   1,   1,   1],\n",
            "       [  0, 225,  43, 269,  75, 648, 386, 412, 293, 556, 264, 692, 577,\n",
            "        288,  89, 441,  16, 514, 330, 489, 264, 720, 375,  55, 414,  74,\n",
            "        269, 325, 271, 296, 265,  88, 441, 284, 428, 272, 274, 403,  76,\n",
            "        592,  16, 263, 390, 301, 406, 405, 385, 279, 464,  71, 307, 286,\n",
            "        319, 482, 577, 324,  81,  93, 403, 291, 567, 415,  87, 587,  81,\n",
            "        266, 272, 401, 318,  18,   2]], dtype=int32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opnbu710_l3W",
        "colab_type": "text"
      },
      "source": [
        "#### Train Model here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmOlN8dq6JBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "572422e3-2d0d-412a-ca3c-b1894b14eb72"
      },
      "source": [
        "configuration = BertConfig.from_pretrained(pretrained_tokenizer_path)\n",
        "bert_model = TFBertModel(config=configuration)\n",
        "\n",
        "input = np.random.randint(0,10, size=(2,3))\n",
        "output = bert_model(input)\n",
        "output[0].shape, output[1].shape"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([2, 3, 128]), TensorShape([2, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rWTpCG17cvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "119e1ca9-c56f-4d20-edee-db8fcb00fee9"
      },
      "source": [
        "bert_model.summary()"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  4022272   \n",
            "=================================================================\n",
            "Total params: 4,022,272\n",
            "Trainable params: 4,022,272\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VucguH_px0iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "1ca132db-f1cc-43b5-a6f8-658640fc0052"
      },
      "source": [
        "from transformers import TFBertForMaskedLM\n",
        "\n",
        "configuration = BertConfig.from_pretrained(\"google/bert_uncased_L-10_H-128_A-2\")\n",
        "model = TFBertForMaskedLM(config=configuration)\n",
        "\n",
        "input = np.random.randint(0,10, size=(2,3))\n",
        "output = model(input)\n",
        "\n",
        "output[0].shape, model.summary()"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_masked_lm_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  5972096   \n",
            "_________________________________________________________________\n",
            "mlm___cls (TFBertMLMHead)    multiple                  4020154   \n",
            "=================================================================\n",
            "Total params: 6,019,386\n",
            "Trainable params: 6,019,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([2, 3, 30522]), None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46IxsV2c_oQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkKGHwBkO_g9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "bd2be205-a68f-4ea0-9668-2bc0472efdd1"
      },
      "source": [
        "model.fit((inputs, attention_masks), labels, epochs=10)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm_14/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm_14/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm_14/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm_14/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm_14/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm_14/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm_14/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm_14/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 10.3212\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.8468\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 9.5937\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.3391\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.1358\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.8983\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 804us/step - loss: 8.6756\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.4460\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.2419\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 874us/step - loss: 8.0255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f75205c7dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF-teZ-_DeVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(inputs)\n",
        "        loss = outputs[0]\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijVSsLT_rW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step, ((inputs, atttention_masks), labels) in enumerate(dataset):\n",
        "    loss = train_step(model, inputs, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csqCJbZn0bBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}