{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "en_file1 = '../data/train.lang1'\n",
    "en_file2 = '../data/train.en.no-punctuation/unaligned.en'\n",
    "fr_file1 = '../data/train.lang2'\n",
    "fr_file2 = '../data/train.fr.tokenized/unaligned.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize all files (whole strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 05:23:03.394031 47112253547520 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../code')\n",
    "import pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guest139/Translation-Team08-IFT6759/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 05:23:19.509272 47112253547520 configuration_utils.py:281] loading configuration file ../tokenizer_data_en_30k/config.json\n",
      "I0418 05:23:19.513222 47112253547520 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "I0418 05:23:19.514664 47112253547520 tokenization_utils.py:420] Model name '../tokenizer_data_en_30k' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../tokenizer_data_en_30k' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0418 05:23:19.558397 47112253547520 tokenization_utils.py:449] Didn't find file ../tokenizer_data_en_30k/added_tokens.json. We won't load it.\n",
      "I0418 05:23:19.583543 47112253547520 tokenization_utils.py:449] Didn't find file ../tokenizer_data_en_30k/tokenizer_config.json. We won't load it.\n",
      "I0418 05:23:19.585057 47112253547520 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/vocab.json\n",
      "I0418 05:23:19.585882 47112253547520 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/merges.txt\n",
      "I0418 05:23:19.586768 47112253547520 tokenization_utils.py:502] loading file None\n",
      "I0418 05:23:19.587572 47112253547520 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/special_tokens_map.json\n",
      "I0418 05:23:19.588368 47112253547520 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "t_fr = pretrained_tokenizer.Tokenizer(language='en', path='../tokenizer_data_en_30k')\n",
    "# t_fr = pretrained_tokenizer.Tokenizer(language='fr', path='../tokenizer_data_fr_30k')\n",
    "# t_fr = pretrained_tokenizer.Tokenizer(language='en', path='../tokenizer_data_en_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474000, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def tokenize_file(filename):\n",
    "    with open(filename) as f:\n",
    "        tokens = []\n",
    "        attention_mask = []\n",
    "        for idx, line in enumerate(f):\n",
    "            encoded = t_fr.encode(line)\n",
    "            tokens += [np.array(encoded['input_ids'])[None,:]]\n",
    "            attention_mask += [np.array(encoded['attention_mask'])[None,:]]\n",
    "    return np.concatenate(tokens,axis=0), np.concatenate(attention_mask,axis=0)\n",
    "\n",
    "x_true, attention_mask = tokenize_file(en_file2)\n",
    "x_true_val, attention_mask_val = tokenize_file(en_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define masking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = t_fr.MAX_LENGTH\n",
    "def mask_tokens(true_tokens, attention_mask):\n",
    "    random_masking_seed = np.random.uniform(0,1,(SENTENCE_LENGTH,)) * attention_mask\n",
    "    \n",
    "    masking_targets = 0.85 < random_masking_seed # 15%\n",
    "    mask_token_targets = np.logical_and(0.85 < random_masking_seed, random_masking_seed < 0.85 + 0.15*0.8) # 80% of 15%\n",
    "    random_token_targets = np.logical_and(1.0 - 0.1*0.15 < random_masking_seed, random_masking_seed < 1.0) # 10% of 15%\n",
    "    \n",
    "    masked_tokens = true_tokens.copy()\n",
    "    masked_tokens[mask_token_targets] = t_fr.tokenizer.mask_token_id\n",
    "    masked_tokens[random_token_targets] = np.random.randint(0,len(t_fr.tokenizer),(random_token_targets.sum(),))\n",
    "\n",
    "    masked_true_tokens = true_tokens.copy()\n",
    "    masked_true_tokens[~masking_targets] = t_fr.tokenizer.pad_token_id\n",
    "    \n",
    "    return masked_tokens, masking_targets, masked_true_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Override model to include masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define masking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_mask = np.zeros(len(t_fr.tokenizer), dtype=np.float32)\n",
    "onehot_mask[t_fr.tokenizer.pad_token_id] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 05:25:25.154355 47112253547520 configuration_utils.py:281] loading configuration file ../code/bert_config_tiny.json\n",
      "I0418 05:25:25.156252 47112253547520 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('../code/bert_config_tiny.json')\n",
    "config.vocab_size = len(t_fr.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = bert_with_mask(config, onehot_mask)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def data_generator_fn():\n",
    "    for x, atn in zip(x_true, attention_mask):\n",
    "        x_train, targets_train, masked_x_true = mask_tokens(x, atn)\n",
    "        yield (x_train, atn, targets_train), masked_x_true\n",
    "\n",
    "# dataset object\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator_fn,\n",
    "    output_types=((tf.int32, tf.bool, tf.bool), tf.int32),\n",
    "    output_shapes=(( tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)) ), tf.TensorShape((SENTENCE_LENGTH,)) )\n",
    ")\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_fn_val():\n",
    "    for x, atn_val in zip(x_true_val, attention_mask_val):\n",
    "        x_train, targets_train, masked_x_true = mask_tokens(x, atn_val)\n",
    "        yield (x_train, atn_val, targets_train), masked_x_true\n",
    "\n",
    "# dataset object\n",
    "dataset_val = tf.data.Dataset.from_generator(\n",
    "    data_generator_fn_val,\n",
    "    output_types=((tf.int32, tf.bool, tf.bool), tf.int32),\n",
    "    output_shapes=(( tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)), tf.TensorShape((SENTENCE_LENGTH,)) ), tf.TensorShape((SENTENCE_LENGTH,)) )\n",
    ")\n",
    "dataset_val = dataset_val.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest139/tr_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0418 05:25:37.774441 47112253547520 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/kernel:0', 'bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "/home/guest139/tr_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0418 05:25:39.714071 47112253547520 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/kernel:0', 'bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.36928"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model\n",
    "model2 = bert_with_mask(config, onehot_mask)\n",
    "model2.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# This initializes the variables used by the optimizers,\n",
    "# as well as any stateful metric variables\n",
    "model2.train_on_batch(dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guest139/Translation-Team08-IFT6759/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "model2.load_weights('ckpts/weights-improvement-10-9.01.hdf5')\n",
    "# model2.load_weights('tinyBERT_en/tinyBERT')\n",
    "# model2.load_weights('ckpts/weights-improvement-20-8.78.hdf5')\n",
    "# model2.load_weights('tinyBERT_fr2/tinyBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ckpts\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "filepath=\"ckpts/weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'ckpts': File exists\n",
      "Epoch 11/25\n",
      "  14813/Unknown - 1930s 130ms/step - loss: 9.0233\n",
      "Epoch 00011: val_loss improved from inf to 9.01219, saving model to ckpts/weights-improvement-11-9.01.hdf5\n",
      "14813/14813 [==============================] - 1955s 132ms/step - loss: 9.0233 - val_loss: 9.0122\n",
      "Epoch 12/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0210\n",
      "Epoch 00012: val_loss improved from 9.01219 to 9.00961, saving model to ckpts/weights-improvement-12-9.01.hdf5\n",
      "14813/14813 [==============================] - 1957s 132ms/step - loss: 9.0210 - val_loss: 9.0096\n",
      "Epoch 13/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0198\n",
      "Epoch 00013: val_loss did not improve from 9.00961\n",
      "14813/14813 [==============================] - 1955s 132ms/step - loss: 9.0198 - val_loss: 9.0122\n",
      "Epoch 14/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0192\n",
      "Epoch 00014: val_loss improved from 9.00961 to 9.00778, saving model to ckpts/weights-improvement-14-9.01.hdf5\n",
      "14813/14813 [==============================] - 1955s 132ms/step - loss: 9.0192 - val_loss: 9.0078\n",
      "Epoch 15/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0173\n",
      "Epoch 00015: val_loss improved from 9.00778 to 9.00720, saving model to ckpts/weights-improvement-15-9.01.hdf5\n",
      "14813/14813 [==============================] - 1954s 132ms/step - loss: 9.0173 - val_loss: 9.0072\n",
      "Epoch 16/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0170\n",
      "Epoch 00016: val_loss improved from 9.00720 to 9.00695, saving model to ckpts/weights-improvement-16-9.01.hdf5\n",
      "14813/14813 [==============================] - 1955s 132ms/step - loss: 9.0170 - val_loss: 9.0069\n",
      "Epoch 17/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0163\n",
      "Epoch 00017: val_loss improved from 9.00695 to 9.00420, saving model to ckpts/weights-improvement-17-9.00.hdf5\n",
      "14813/14813 [==============================] - 1958s 132ms/step - loss: 9.0163 - val_loss: 9.0042\n",
      "Epoch 18/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0153\n",
      "Epoch 00018: val_loss did not improve from 9.00420\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0153 - val_loss: 9.0049\n",
      "Epoch 19/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0145\n",
      "Epoch 00019: val_loss did not improve from 9.00420\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0145 - val_loss: 9.0059\n",
      "Epoch 20/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0144\n",
      "Epoch 00020: val_loss did not improve from 9.00420\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0144 - val_loss: 9.0060\n",
      "Epoch 21/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0139\n",
      "Epoch 00021: val_loss improved from 9.00420 to 9.00325, saving model to ckpts/weights-improvement-21-9.00.hdf5\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0139 - val_loss: 9.0033\n",
      "Epoch 22/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0132\n",
      "Epoch 00022: val_loss improved from 9.00325 to 9.00320, saving model to ckpts/weights-improvement-22-9.00.hdf5\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0132 - val_loss: 9.0032\n",
      "Epoch 23/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0128\n",
      "Epoch 00023: val_loss did not improve from 9.00320\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0128 - val_loss: 9.0034\n",
      "Epoch 24/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0122\n",
      "Epoch 00024: val_loss improved from 9.00320 to 9.00260, saving model to ckpts/weights-improvement-24-9.00.hdf5\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0122 - val_loss: 9.0026\n",
      "Epoch 25/25\n",
      "14812/14813 [============================>.] - ETA: 0s - loss: 9.0125\n",
      "Epoch 00025: val_loss did not improve from 9.00260\n",
      "14813/14813 [==============================] - 1962s 132ms/step - loss: 9.0125 - val_loss: 9.0031\n"
     ]
    }
   ],
   "source": [
    "hist = model2.fit(dataset, validation_data=dataset_val, initial_epoch=10, epochs=25, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [9.023326345129858,\n",
       "  9.0209677878915,\n",
       "  9.019792253711556,\n",
       "  9.019181977284106,\n",
       "  9.017312573976154,\n",
       "  9.017025964986424,\n",
       "  9.01628955329219,\n",
       "  9.015283525748595,\n",
       "  9.014510984267867,\n",
       "  9.014428088868218,\n",
       "  9.013857906035733,\n",
       "  9.013160177753948,\n",
       "  9.012782871133668,\n",
       "  9.012200083221565,\n",
       "  9.0124870794755],\n",
       " 'val_loss': [9.012193613274153,\n",
       "  9.009608476660972,\n",
       "  9.012180081633634,\n",
       "  9.007779406946758,\n",
       "  9.007198236709417,\n",
       "  9.006946231043615,\n",
       "  9.004197935725367,\n",
       "  9.004905312560325,\n",
       "  9.005924216536588,\n",
       "  9.006016800569933,\n",
       "  9.003250912178395,\n",
       "  9.003198166226232,\n",
       "  9.003413519193959,\n",
       "  9.002597262693007,\n",
       "  9.003135553626127]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tinyBERT_fr2\n",
    "!mkdir tinyBERT_fr2\n",
    "model2.save_weights('tinyBERT_fr2/tinyBERT', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr_env_ipyk",
   "language": "python",
   "name": "tr_env_ipyk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
