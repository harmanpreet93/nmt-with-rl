{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0416 19:15:27.636775 47039360943104 file_utils.py:57] TensorFlow version 2.1.0 available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForMaskedLM, BertConfig\n",
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0416 19:15:41.362657 47039360943104 configuration_utils.py:281] loading configuration file ../tokenizer_data_en_30k/config.json\n",
      "I0416 19:15:41.366433 47039360943104 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "I0416 19:15:41.369457 47039360943104 tokenization_utils.py:420] Model name '../tokenizer_data_en_30k' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../tokenizer_data_en_30k' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0416 19:15:41.416436 47039360943104 tokenization_utils.py:449] Didn't find file ../tokenizer_data_en_30k/added_tokens.json. We won't load it.\n",
      "I0416 19:15:41.466492 47039360943104 tokenization_utils.py:449] Didn't find file ../tokenizer_data_en_30k/tokenizer_config.json. We won't load it.\n",
      "I0416 19:15:41.470030 47039360943104 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/vocab.json\n",
      "I0416 19:15:41.472347 47039360943104 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/merges.txt\n",
      "I0416 19:15:41.474689 47039360943104 tokenization_utils.py:502] loading file None\n",
      "I0416 19:15:41.477090 47039360943104 tokenization_utils.py:502] loading file ../tokenizer_data_en_30k/special_tokens_map.json\n",
      "I0416 19:15:41.479802 47039360943104 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../code')\n",
    "import pretrained_tokenizer\n",
    "\n",
    "# t_fr = pretrained_tokenizer.Tokenizer(language='fr', path='../tokenizer_data_fr_30k')\n",
    "t_fr = pretrained_tokenizer.Tokenizer(language='en', path='../tokenizer_data_en_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "onehot_mask = np.zeros(len(t_fr.tokenizer), dtype=np.float32)\n",
    "onehot_mask[t_fr.tokenizer.pad_token_id] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0416 19:15:42.284523 47039360943104 configuration_utils.py:281] loading configuration file ../code/bert_config_tiny.json\n",
      "I0416 19:15:42.288191 47039360943104 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model\n",
    "config = BertConfig.from_pretrained('../code/bert_config_tiny.json')\n",
    "config.vocab_size = len(t_fr.tokenizer)\n",
    "\n",
    "new_model = bert_with_mask(config, onehot_mask)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-3, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "new_model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest139/tr_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0416 19:15:50.788982 47039360943104 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "/home/guest139/tr_env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0416 19:15:52.709235 47039360943104 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'bert_with_mask/tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.209963"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This initializes the variables used by the optimizers,\n",
    "# as well as any stateful metric variables\n",
    "SENTENCE_LENGTH = t_fr.MAX_LENGTH\n",
    "new_model.train_on_batch((tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool)), (tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2ac8476c2ad0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state of the old model\n",
    "new_model.load_weights('tinyBERT_fr2/tinyBERT')\n",
    "# new_model.load_weights('ckpts/weights-improvement-49-9.00.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04424642,  0.06636524,  0.18779634, ...,  0.12359824,\n",
       "         0.20507875,  0.00632676],\n",
       "       [ 0.02300078,  0.04526478,  0.1138289 , ...,  0.18306506,\n",
       "         0.36607835,  0.08191235],\n",
       "       [ 0.2874621 , -0.02671925,  0.21807905, ...,  0.41321686,\n",
       "         0.36394203,  0.05828147],\n",
       "       ...,\n",
       "       [-0.06198958,  0.34959   ,  0.22715902, ...,  0.14978306,\n",
       "         0.23692305,  0.12474256],\n",
       "       [ 0.01955202,  0.23921558,  0.13941337, ...,  0.24168685,\n",
       "         0.4770586 ,  0.13954757],\n",
       "       [-0.13496612,  0.40748537,  0.1220889 , ...,  0.16994001,\n",
       "         0.24111958,  0.00428365]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = new_model.bert.weights[0].numpy()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "#     'tf_bert_for_masked_lm_epoch_49_loss_9.00_EN_30k_position.npy',\n",
    "    'tf_bert_for_masked_lm_epoch_40_loss_8.78_FR_30k_position.npy',\n",
    "    new_model.bert.weights[1].numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights2 = np.load('tf_bert_for_masked_lm_epoch_49_loss_9.00_EN_30k_position.npy')\n",
    "weights2 = np.load('tf_bert_for_masked_lm_epoch_40_loss_8.78_FR_30k_position.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (new_model.bert.weights[1].numpy() == weights2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 128)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02549606, -0.34520984, -0.11114695, ...,  0.00919494,\n",
       "        -0.02353353,  0.17783244],\n",
       "       [-0.08381344, -0.24735476, -0.02676942, ...,  0.16489263,\n",
       "        -0.04131527, -0.02563108],\n",
       "       [-0.11810085, -0.20583269, -0.05165746, ...,  0.3128433 ,\n",
       "         0.07927801, -0.19550008],\n",
       "       ...,\n",
       "       [-0.02872846, -0.00938647,  0.00218509, ...,  0.01731755,\n",
       "         0.03368582, -0.00059017],\n",
       "       [ 0.00795601,  0.00063555, -0.02629197, ...,  0.00309679,\n",
       "        -0.01845641,  0.02721068],\n",
       "       [ 0.00711415,  0.02857519, -0.03790667, ...,  0.00750328,\n",
       "        -0.0061818 ,  0.00392741]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tr_env_ipyk",
   "language": "python",
   "name": "tr_env_ipyk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
