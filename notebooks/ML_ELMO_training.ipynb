{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to train ELMo model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source from:\n",
    "* https://github.com/allenai/bilm-tf\n",
    "* https://appliedmachinelearning.blog/2019/11/30/training-elmo-from-scratch-on-custom-data-set-for-generating-embeddings-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize load the unaligned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the provided tokenizer to add space between tokens, so we will load the tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DIRECTORY_URL = \"data/train/\"\n",
    "FILE_NAMES = [\"unalignedtok.en\", \"unalignedtok.fr\"]\n",
    "\n",
    "unaligned_en = []\n",
    "unaligned_fr = []\n",
    "\n",
    "with open(os.path.join(DIRECTORY_URL, FILE_NAMES[0]), 'r', encoding=\"UTF-8\") as en_file:\n",
    "    for line in en_file.readlines():\n",
    "        line = line.rstrip().split('\\n')\n",
    "        unaligned_en.append(line[0])\n",
    "    en_file.close()\n",
    "    \n",
    "with open(os.path.join(DIRECTORY_URL, FILE_NAMES[1]), 'r', encoding=\"UTF-8\") as fr_file:\n",
    "    for line in fr_file.readlines():\n",
    "        line = line.rstrip().split('\\n')\n",
    "        unaligned_fr.append(line[0])\n",
    "    fr_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474000, 105395)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unaligned_en), len(unaligned_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for the second phase of the trials we just had different sizes , small , medium , large and extra - large . it 's true .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaligned_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nous n’ aurions pas pu dégager d’ accord sur un calendrier de conclusion de la cig sans l’ engagement politique de mes collègues du conseil européen .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unaligned_fr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown the unaligned dataset\n",
    "As the training requires multiple files with one text sentence per line, we will create 79K training files by writing 6 sentences per file. After running the below python snippet, we get 79K files in train directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainset(train_folder, dataset):\n",
    "    # create train folder\n",
    "    if not os.path.exists(train_folder):\n",
    "        os.makedirs(train_folder)\n",
    "    \n",
    "    # breakdow the dataset\n",
    "    for i in range(0,dataset.shape[0],6):\n",
    "        text = \"\\n\".join(dataset[i:i+6].tolist())\n",
    "        fp = open(train_folder+str(i)+\".txt\",\"w\", encoding='UTF-8')\n",
    "        fp.write(text)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script on our 2 datasets\n",
    "create_trainset(\"swb_en\\\\train\\\\\", np.array(unaligned_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script on our 2 datasets\n",
    "create_trainset(\"swb_fr\\\\train\\\\\", np.array(unaligned_fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(dataset, train_folder):\n",
    "    texts = \" \".join(dataset.tolist())\n",
    "    words = texts.split(\" \")\n",
    "    print(\"Number of tokens in Training data = \",len(words))\n",
    "    dictionary = Counter(words)\n",
    "    print(\"Size of Vocab\",len(dictionary))\n",
    "    sorted_vocab = [\"<S>\",\"</S>\",\"<UNK>\"]\n",
    "    sorted_vocab.extend([pair[0] for pair in dictionary.most_common()])\n",
    "\n",
    "    text = \"\\n\".join(sorted_vocab)\n",
    "    fp = open(train_folder+\"\\\\vocab.txt\",\"w\", encoding='UTF-8')\n",
    "    fp.write(text)\n",
    "    fp.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in Training data =  9814295\n",
      "Size of Vocab 60023\n"
     ]
    }
   ],
   "source": [
    "en_vocab = get_vocabulary(np.array(unaligned_en), \"swb_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in Training data =  2389468\n",
      "Size of Vocab 47190\n"
     ]
    }
   ],
   "source": [
    "fr_vocab = get_vocabulary(np.array(unaligned_fr), \"swb_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the biLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training was done on the cluster, here are the followed steps:\n",
    "* Install bilm package from their github depository: https://github.com/allenai/bilm-tf\n",
    "* Put the train data, the generated vocabulary and the hyperparameters (options.json) under the same folder (ex: swb)\n",
    "* Configure the hyperparameters (e.g. projection dim) in options.json and put them under checkpoint folder\n",
    "* Run the following script: python bin/train_elmo.py --train_prefix='swb/train/*' --vocab_file 'swb/vocab.txt' --save_dir 'swb/checkpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we have to convert the tensoflow checkpoints to hdf5 weights, by running this script:\n",
    "* python bin/dump_weights.py --save_dir 'swb/checkpoint' --outfile 'swb/swb_weights.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ELMo embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the weights generated, we are ready to generate ELMo embeddings for a given sequence, by calling the following method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befor calling this method, we have to make final adjustement:\n",
    "* Keep the dumped weights file in newly created model folder.\n",
    "* Create an options.json file for the newly trained model in same folder.\n",
    "* It is important to always set n_characters to 262 after training.\n",
    "* Keep vocab.txt in model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_emb(input_seq, max_length, options_file, weight_file, vocab_file):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    print('getting the context of the sequence :', input_seq)\n",
    "    # Create a Batcher to map text to character ids.\n",
    "    batcher = Batcher(vocab_file, 50)\n",
    " \n",
    "    # Input placeholders to the biLM.\n",
    "    context_character_ids = tf.compat.v1.placeholder('int32', shape=(None, None, 50))\n",
    " \n",
    "    # Build the biLM graph.\n",
    "    bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    " \n",
    "    # Get ops to compute the LM embeddings.\n",
    "    context_embeddings_op = bilm(context_character_ids)\n",
    "     \n",
    "    # Get an op to compute ELMo (weighted average of the internal biLM layers)\n",
    "    elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    "    \n",
    "    # Now we can compute embeddings.\n",
    "    #print(\"get elmo: input_seq=\",input_seq)\n",
    "    tokenized_context = [input_seq.split()] # for sentence in input_seq]\n",
    "    #print(tokenized_context)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # It is necessary to initialize variables once before running inference.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    " \n",
    "        # Create batches of data.\n",
    "        context_ids = batcher.batch_sentences(tokenized_context)\n",
    "        #print(\"Shape of context ids = \", context_ids.shape)\n",
    " \n",
    "        # Compute ELMo representations (here for the input only, for simplicity).\n",
    "        elmo_context_input_ = sess.run(\n",
    "            elmo_context_input['weighted_op'],\n",
    "            feed_dict={context_character_ids: context_ids}\n",
    "        )\n",
    "    # Pad the output to max sequence length of the model\n",
    "    elmo_emb = pad_sequences(elmo_context_input_, maxlen=max_length, padding='post', dtype='float32')\n",
    "    #print(\"Shape of generated embeddings = \",elmo_emb.shape)\n",
    "    return elmo_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters:\n",
    "input_seq = unaligned_en[0]\n",
    "max_length = len(unaligned_en[0])\n",
    "options_file = \"ELMo/swb_en/options_eval.json\"\n",
    "weight_file = \"ELMo/swb_en/swb_weights_en.hdf5\"\n",
    "vocab_file = \"ELMo/swb_en/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting the context of the sequence : for the second phase of the trials we just had different sizes , small , medium , large and extra - large . it 's true .\n",
      "USING SKIP CONNECTIONS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.42958167,  0.6997375 , -1.1938176 , ...,  1.2128806 ,\n",
       "         -0.5734399 , -0.07925927],\n",
       "        [ 0.18870763,  0.75108993,  0.35382527, ...,  0.55428684,\n",
       "         -1.0110341 , -0.6422628 ],\n",
       "        [ 1.8181396 ,  0.79422575, -0.23697373, ...,  0.43018457,\n",
       "         -2.3241005 , -2.5005286 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_elmo_emb(input_seq, max_length, options_file, weight_file, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
