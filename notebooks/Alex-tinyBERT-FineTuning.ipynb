{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT to improve existing translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Collecting tensorflow-gpu\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.25.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.18.2)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-gpu)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (0.1.8)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (0.7.1)\n",
      "Collecting protobuf>=3.8.0 (from tensorflow-gpu)\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow-gpu)\n",
      "Requirement already satisfied: six>=1.12.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from tensorflow-gpu) (1.13.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/site-packages/wheel-0.33.4-py3.7.egg (from tensorflow-gpu) (0.33.4)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu)\n",
      "Requirement already satisfied: gast==0.2.2 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied: h5py in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow-gpu) (40.8.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.23.0)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
      "\u001b[31mERROR: tensorboard 2.1.0 has requirement setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: opt-einsum, protobuf, astor, markdown, google-auth, google-auth-oauthlib, werkzeug, tensorboard, termcolor, tensorflow-gpu\n",
      "Successfully installed astor-0.8.1 google-auth-1.11.0 google-auth-oauthlib-0.4.1 markdown-3.2 opt-einsum-2.3.2 protobuf-3.11.3 tensorboard-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 werkzeug-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/s3transfer/botocore\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore==1.15.37) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore==1.15.37) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from botocore==1.15.37) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore==1.15.37) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.15.37) (1.13.0)\n",
      "Building wheels for collected packages: botocore\n",
      "  Building wheel for botocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-27y_f2qy/wheels/fe/ab/37/f8ebb3c769c11a806041a139b0ab3964015975a5bb08071dc6\n",
      "Successfully built botocore\n",
      "Installing collected packages: botocore\n",
      "  Found existing installation: botocore 1.15.37\n",
      "    Uninstalling botocore-1.15.37:\n",
      "      Successfully uninstalled botocore-1.15.37\n",
      "Successfully installed botocore-1.15.37\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/s3transfer\n",
      "Requirement already satisfied: botocore<2.0a.0,>=1.12.36 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from s3transfer==0.3.3) (1.15.37)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (1.25.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (0.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0a.0,>=1.12.36->s3transfer==0.3.3) (1.13.0)\n",
      "Building wheels for collected packages: s3transfer\n",
      "  Building wheel for s3transfer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-fsq6w3nv/wheels/21/b5/7f/8dc1ac8a07cf30a6f8e4c8c6af478ed702be97967619f38b8b\n",
      "Successfully built s3transfer\n",
      "Installing collected packages: s3transfer\n",
      "  Found existing installation: s3transfer 0.3.3\n",
      "    Uninstalling s3transfer-0.3.3:\n",
      "      Successfully uninstalled s3transfer-0.3.3\n",
      "Successfully installed s3transfer-0.3.3\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/sacremoses\n",
      "Requirement already satisfied: regex in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses==0.0.38) (2019.11.1)\n",
      "Requirement already satisfied: six in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from sacremoses==0.0.38) (1.13.0)\n",
      "Requirement already satisfied: click in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses==0.0.38) (7.1.1)\n",
      "Requirement already satisfied: joblib in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses==0.0.38) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses==0.0.38) (4.40.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-2gl4gkm9/wheels/e0/47/84/d17295db70c2c5f374b2f64c5b161ca6324e0dff3d92eaf80c\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses\n",
      "  Found existing installation: sacremoses 0.0.38\n",
      "    Uninstalling sacremoses-0.0.38:\n",
      "      Successfully uninstalled sacremoses-0.0.38\n",
      "Successfully installed sacremoses-0.0.38\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/transformers\n",
      "Requirement already satisfied: numpy in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (1.18.2)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (1.12.31)\n",
      "Requirement already satisfied: filelock in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (4.40.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (2019.11.1)\n",
      "Requirement already satisfied: sentencepiece in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (0.1.82)\n",
      "Requirement already satisfied: sacremoses in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from transformers==2.8.0) (0.0.38)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (1.15.37)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2019.11.28)\n",
      "Requirement already satisfied: joblib in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: click in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (7.1.1)\n",
      "Requirement already satisfied: six in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (1.13.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/ipykernel/5.1.3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.8.0) (2.8.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-lgg655wx/wheels/63/32/25/2e434bbe27e8807fd0665b43ecac20f96b0d8d8124eb8fc97f\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Found existing installation: transformers 2.8.0\n",
      "    Uninstalling transformers-2.8.0:\n",
      "      Successfully uninstalled transformers-2.8.0\n",
      "Successfully installed transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /home/guest139/s3transfer/botocore\n",
    "!pip install --no-index /home/guest139/s3transfer\n",
    "!pip install --no-index /home/guest139/sacremoses\n",
    "!pip install --no-index /home/guest139/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 14:32:19.699036 47207483892736 configuration_utils.py:281] loading configuration file ../tokenizer_data_fr_30k/config.json\n",
      "I0413 14:32:19.700877 47207483892736 configuration_utils.py:319] Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "I0413 14:32:19.701893 47207483892736 tokenization_utils.py:420] Model name '../tokenizer_data_fr_30k' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../tokenizer_data_fr_30k' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0413 14:32:19.711822 47207483892736 tokenization_utils.py:449] Didn't find file ../tokenizer_data_fr_30k/added_tokens.json. We won't load it.\n",
      "I0413 14:32:19.717243 47207483892736 tokenization_utils.py:449] Didn't find file ../tokenizer_data_fr_30k/tokenizer_config.json. We won't load it.\n",
      "I0413 14:32:19.718751 47207483892736 tokenization_utils.py:502] loading file ../tokenizer_data_fr_30k/vocab.json\n",
      "I0413 14:32:19.719583 47207483892736 tokenization_utils.py:502] loading file ../tokenizer_data_fr_30k/merges.txt\n",
      "I0413 14:32:19.720366 47207483892736 tokenization_utils.py:502] loading file None\n",
      "I0413 14:32:19.721250 47207483892736 tokenization_utils.py:502] loading file ../tokenizer_data_fr_30k/special_tokens_map.json\n",
      "I0413 14:32:19.722072 47207483892736 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../code')\n",
    "import pretrained_tokenizer\n",
    "\n",
    "t_fr = pretrained_tokenizer.Tokenizer(language='fr', path='../tokenizer_data_fr_30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM, BertConfig\n",
    "class bert_with_mask(tf.keras.Model):\n",
    "    def __init__(self, config, onehot_mask):\n",
    "        super(bert_with_mask, self).__init__()\n",
    "        self.bert = TFBertForMaskedLM(config)\n",
    "        self.onehot_mask = onehot_mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mask = inputs[-1] # unpack mask from inputs\n",
    "        inputs = inputs[:-1]\n",
    "        outputs = self.bert(inputs)[0]\n",
    "        \n",
    "        outputs = tf.where(mask[:,:,None], outputs, self.onehot_mask[None,None,:])\n",
    "        \n",
    "        return (outputs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "onehot_mask = np.zeros(len(t_fr.tokenizer), dtype=np.float32)\n",
    "onehot_mask[t_fr.tokenizer.pad_token_id] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 14:32:21.458865 47207483892736 configuration_utils.py:281] loading configuration file ../code/bert_config_tiny.json\n",
      "I0413 14:32:21.460298 47207483892736 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model\n",
    "config = BertConfig.from_pretrained('../code/bert_config_tiny.json')\n",
    "config.vocab_size = len(t_fr.tokenizer)\n",
    "\n",
    "new_model = bert_with_mask(config, onehot_mask)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-3, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "new_model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/guest139.358248.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0413 14:32:24.196261 47207483892736 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/kernel:0', 'bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "/localscratch/guest139.358248.0/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0413 14:32:26.332130 47207483892736 optimizer_v2.py:1043] Gradients do not exist for variables ['bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/kernel:0', 'bert_with_mask_1/tf_bert_for_masked_lm_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.166824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This initializes the variables used by the optimizers,\n",
    "# as well as any stateful metric variables\n",
    "SENTENCE_LENGTH = t_fr.MAX_LENGTH\n",
    "new_model.train_on_batch((tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool), tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool)), (tf.zeros(shape=(1,SENTENCE_LENGTH),dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state of the old model\n",
    "new_model.load_weights('ckpts/weights-improvement-20-8.78.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict probs with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guest139/Translation-Team08-IFT6759/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# fr_file1 = '../data/train.lang2.sub'\n",
    "fr_file1 = '../data/test_harman/test_fr_predictions.txt'\n",
    "# fr_file1 = '../data/test_harman/test_fr_predictions2.txt'\n",
    "# fr_file1 = '../data/test_harman/test_fr_predictions3.txt'\n",
    "# fr_file1 = '../data/test_harman/test_fr_predictions4.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(filename):\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        attention_mask = []\n",
    "        for idx, line in enumerate(f):\n",
    "            encoded = t_fr.encode(line)\n",
    "            tokens += [np.array(encoded['input_ids'])[None,:]]\n",
    "            attention_mask += [np.array(encoded['attention_mask'])[None,:]]\n",
    "    return np.concatenate(tokens,axis=0), np.concatenate(attention_mask,axis=0)\n",
    "\n",
    "x_true, attention_mask = tokenize_file(fr_file1)\n",
    "attention_mask = attention_mask.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 64, 30000), dtype=float32, numpy=\n",
       "array([[[-2.5675648e+01,  9.5021229e+00, -1.5938702e+01, ...,\n",
       "         -2.6073385e+01, -2.5940208e+01, -2.2952614e+01],\n",
       "        [-2.6017750e+01, -7.3445654e+00, -1.4740413e+01, ...,\n",
       "         -2.5291624e+01, -2.4206230e+01, -2.2025677e+01],\n",
       "        [-2.1415951e+01, -8.9850197e+00, -1.4591470e+01, ...,\n",
       "         -2.2855679e+01, -2.4495766e+01, -1.7723272e+01],\n",
       "        ...,\n",
       "        [-1.7026737e+01, -1.7658922e+01, -2.5757554e+00, ...,\n",
       "         -1.4725203e+01, -1.5865658e+01, -1.8158695e+01],\n",
       "        [-1.4984193e+01, -1.3680983e+01, -4.9321651e-03, ...,\n",
       "         -1.5642763e+01, -1.6222641e+01, -1.7623980e+01],\n",
       "        [-1.6912313e+01, -9.8356638e+00,  1.6491385e+01, ...,\n",
       "         -2.0540480e+01, -1.8524321e+01, -2.1160803e+01]]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = new_model((x_true[0,None], attention_mask[0,None], tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool)))\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
       "array([    1,   225,    36,   269,    11,   326,  1045,   317,   363,\n",
       "         287,    11,   613,   360,   302,   618, 11952,   319,   289,\n",
       "        1374,   276,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,   360,   276,     2,     2,     2,     2,     2,     2,\n",
       "         556,   276,     2,     2,     2,     2,     2,     2,   324,\n",
       "           2,     2,    11,   359,   359,   360,   289,   311,     2,\n",
       "           2])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amax_pred = tf.squeeze(tf.argmax(pred[0], axis=2))\n",
    "amax_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C' est ainsi que nous n' avons pas et même imaginons la coopération .\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_fr.decode(amax_pred[attention_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_line(x, atn):\n",
    "    pred = new_model((x[None,:], atn[None,:], tf.ones(shape=(1,SENTENCE_LENGTH),dtype=tf.bool)))\n",
    "    amax_pred = tf.squeeze(tf.argmax(pred[0], axis=2))\n",
    "    out_line = t_fr.decode(amax_pred[atn])\n",
    "    return out_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for x, atn in zip(x_true, attention_mask):\n",
    "    lines += [correct_line(x, atn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_file =  '../data/train.lang2.out'\n",
    "out_file = '../data/test_harman/test_fr_predictions.out'\n",
    "# out_file = '../data/test_harman/test_fr_predictions2.out'\n",
    "# out_file = '../data/test_harman/test_fr_predictions3.out'\n",
    "# out_file = '../data/test_harman/test_fr_predictions4.out'\n",
    "\n",
    "with open(out_file, 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/portalocker\n",
      "Building wheels for collected packages: portalocker\n",
      "  Building wheel for portalocker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-a7mrzrqf/wheels/1a/2a/f3/d2865b87939e065fc1c0d65412a9822a82942b171fae4b8b4f\n",
      "Successfully built portalocker\n",
      "\u001b[31mERROR: sacrebleu 1.4.3 requires typing, which is not installed.\u001b[0m\n",
      "Installing collected packages: portalocker\n",
      "  Found existing installation: portalocker 1.7.0\n",
      "    Uninstalling portalocker-1.7.0:\n",
      "      Successfully uninstalled portalocker-1.7.0\n",
      "Successfully installed portalocker-1.7.0\n",
      "Ignoring pip: markers 'python_version < \"3\"' don't match your environment\n",
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Processing /home/guest139/sacrebleu\n",
      "Collecting typing (from sacrebleu==1.4.3)\n",
      "Requirement already satisfied: portalocker in /localscratch/guest139.358248.0/lib/python3.7/site-packages (from sacrebleu==1.4.3) (1.7.0)\n",
      "Building wheels for collected packages: sacrebleu\n",
      "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-irllmx5h/wheels/00/46/0a/0f56decffe5daa5de7d4c52af133903ed97462c76fb19064ec\n",
      "Successfully built sacrebleu\n",
      "Installing collected packages: typing, sacrebleu\n",
      "  Found existing installation: sacrebleu 1.4.3\n",
      "    Uninstalling sacrebleu-1.4.3:\n",
      "      Successfully uninstalled sacrebleu-1.4.3\n",
      "Successfully installed sacrebleu-1.4.3 typing-3.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /home/guest139/portalocker\n",
    "!pip install --no-index /home/guest139/sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling typing-3.6.6:\n",
      "  Successfully uninstalled typing-3.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall --yes typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: I need either a predefined test set (-t) or a list of references\n",
      "sacreBLEU: The available test sets are:\n",
      "           wmt19/dev: Development data for tasks new to 2019.\n",
      "               wmt19: Official evaluation data.\n",
      "       wmt18/test-ts: Official evaluation sources with extra test sets interleaved.\n",
      "           wmt18/dev: Development data (Estonian<>English).\n",
      "               wmt18: Official evaluation data.\n",
      "       wmt17/tworefs: Systems with two references.\n",
      "            wmt17/ms: Additional Chinese-English references from Microsoft Research.\n",
      "      wmt17/improved: Improved zh-en and en-zh translations.\n",
      "           wmt17/dev: Development sets released for new languages in 2017.\n",
      "             wmt17/B: Additional reference for EN-FI and FI-EN.\n",
      "               wmt17: Official evaluation data.\n",
      "       wmt16/tworefs: EN-FI with two references.\n",
      "           wmt16/dev: Development sets released for new languages in 2016.\n",
      "             wmt16/B: Additional reference for EN-FI.\n",
      "               wmt16: Official evaluation data.\n",
      "               wmt15: Official evaluation data.\n",
      "          wmt14/full: Evaluation data released after official evaluation for further research.\n",
      "               wmt14: Official evaluation data.\n",
      "               wmt13: Official evaluation data.\n",
      "               wmt12: Official evaluation data.\n",
      "               wmt11: Official evaluation data.\n",
      "               wmt10: Official evaluation data.\n",
      "               wmt09: Official evaluation data.\n",
      "            wmt08/nc: Official evaluation data (news commentary).\n",
      "      wmt08/europarl: Official evaluation data (Europarl).\n",
      "               wmt08: Official evaluation data.\n",
      "            mtnt2019: Test set for the WMT 19 robustness shared task\n",
      "       mtnt1.1/valid: Validation data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
      "       mtnt1.1/train: Training data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
      "        mtnt1.1/test: Test data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
      "     iwslt17/tst2016: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2015: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2014: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2013: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2012: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2011: Development data for IWSLT 2017.\n",
      "     iwslt17/tst2010: Development data for IWSLT 2017.\n",
      "     iwslt17/dev2010: Development data for IWSLT 2017.\n",
      "             iwslt17: Official evaluation data for IWSLT.\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "final avg bleu score: 7.73\n",
      "final avg bleu score: 9.87\n",
      "final avg bleu score: 9.78\n",
      "final avg bleu score: 10.63\n"
     ]
    }
   ],
   "source": [
    "from evaluator import compute_bleu\n",
    "print('Baseline')\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions.txt', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions2.txt', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions3.txt', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions4.txt', print_all_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Masked LM:\n",
      "final avg bleu score: 7.75\n",
      "final avg bleu score: 9.85\n",
      "final avg bleu score: 9.74\n",
      "final avg bleu score: 10.63\n"
     ]
    }
   ],
   "source": [
    "print('With Masked LM:')\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions.out', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions2.out', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions3.out', print_all_scores=False)\n",
    "compute_bleu('../data/test_harman/test_fr.txt', '../data/test_harman/test_fr_predictions4.out', print_all_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C' est mon village que nous et nous pouvons faire ce point de coopération avec l' espace .\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/test_harman/test_fr_predictions3.out'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
